\documentclass[../Main.tex]{subfiles}
\begin{document}
\section{Crawler}
The Crawler is responsible for retrieving the GitHub projects. The Crawler gathers projects by their project ID in ascending order. The Crawler is also responsible for retrieving project metadata from GitHub projects. In practice, all communication with the GitHub API is done through the Crawler.\\
All data retrieved by the Crawler is only returned in code and not displayed in the console. The only things that will be printed to the console are error/warning/info messages and indications of how far and how much the crawler has crawled.

\newpar{Crawling GitHub}
Whenever the Crawler crawls, it crawls one page of the GitHub list. This one page contains 100 URLs. We communicate with GitHub via libcurl \cite{curl}. However, as a lot of URLs have become invalid over time (e.g. the project has been deleted or set to private), we do not always return 100 URLs to projects. We also do not crawl the URL of a project if the project contains zero bytes that we can parse in our parser. We check the amount of parseable bytes (bytes which are in files with extensions of languages that we can parse) a project has by doing an additional GitHub query asking for the project's languages list.
~\\
~\\
The function responsible for crawling GitHub is \texttt{crawlRepositories(url, start, username, token)}. The start parameter in the \texttt{crawlRepositories()} repositories function indicates from which project ID the Crawler will start crawling. It does this by going to the URL \url{https://api.github.com/repositories?since=start} where \texttt{start} is the initial project ID. For example, to start from project ID 9001, you would write \url{https://api.github.com/repositories?since=9001}. If you want to find the ID of a project given its URL, you can do so by replacing \texttt{github.com} with \texttt{api.github.com/repos/}. For example, \url{https://github.com/torvalds/linux} would become \url{https://api.github.com/repos/torvalds/linux}. The "\texttt{id}" entry indicates the project ID of this project (which is 2325298 in this case).
~\\
~\\
A standalone project of the crawler is delivered with the final product, and the \texttt{main} file in the standalone project calls the \texttt{crawlRepositories()} function, defining the username and token to use with the GitHub API. To execute the standalone with a working token you have to set the token variable to a GitHub API token.
\newpar{GitHub API tokens}
For crawling GitHub we use the standard GitHub API (the REST version) with GitHub API tokens. The default amount of GitHub API calls we can make per hour is 60, but by using a GitHub API token we can extend this number to 5000. If we were to crawl non stop we could crawl 24 pages of the GitHub list per hour. \\
When the Crawler is done crawling a page it will return a list of projects with their priority measures and it will also return a number indicating the last project ID we have found. As there are quite a lot of gaps between the project IDs, 
the number from which we have to start crawling next time needs to be returned by the worker node that just crawled to prevent duplicate projects in the job queue. 
~\\
~\\
A GitHub token can be generated by going to \url{https://github.com/}, signing in, then clicking on your profile picture in the top right corner and then selecting "Settings". Then click on "Developer settings" on the left hand side and select "Personal access tokens" and finally "Generate new token". You don't have to tick any boxes, just clicking "Generate token" at the bottom of the page will do fine. Make sure to save the token you receive after this as it will not be shown to you again by GitHub. See \href{https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token}{this} for additional information.
\newpage
\newpar{Priority measure}
On every project we crawl we put a so called "priority measure". This is an integer which we return together with every URL we retrieve which indicates the priority it should get in the job queue: the lower its priority measure, the sooner it should be parsed. In order to calculate this priority measure we need the number of stars, the date at which we crawled this project, the amount of parseable bytes in the project and the percentage of the total amount of parseable bytes to the total amount of bytes of code in the project. This requires two GitHub API calls per project: one to retrieve the number of stars and one to retrieve the information about the languages. This means that per page of GitHub information we need 201 GitHub calls (we need one call for the page containing the projects). The formula with which we calculate the priority is the following.
\[
T - 20.000.000 \cdot P \cdot \lg(S+1) \cdot \lg(\lg(B+1)+1).
\]
Here, $T$ represents the time of uploading the project, $P$ indicates the fraction of bytes that are contained in files written in languages which our parser can parse, $S$ is the number of stars the project has and $B$ is the total number of parseable bytes. The crawler calculates $20.000.000 \cdot P \cdot \lg(S+1) \cdot \lg(\lg(B+1))$ and sets this as the priority. When the database API receives a new job, created by the crawler, it will subtract this priority from the current timestamp to arrive at the $T - 20.000.000 \cdot P \cdot \lg(S+1) \cdot \lg(\lg(B+1))$ priority. This is done server-side to make sure all timestamps are retrieved on synchronised machines. The lower the priority score of a project, the sooner it will be retrieved from the job queue.\\

We need to clone an entire repository before we can parse the parseable files, so a repository with a small fraction of parseable lines will take relatively longer to process per line, than a repository for which we can parse almost every line. Therefore, the priority measure we chose depends on $P$ in a linear way. Since the number of stars and number of parseable bytes is unbounded (unlike $P$, which is bounded between $0$ and $1$), we scale these numbers by taking a base $2$ logarithm. This is also because these are more a measure of popularity than parsing speed, unlike $P$. Since the number of stars is more of an indication of importance than the number of parseable lines, we take a nested double logarithm of $B$. We add $1$ to prevent the logarithm to dip below $0$. Finally, the priority depends on the time of uploading in a linear way, since we want unpopular projects which are in the queue for a while to eventually get moved to the front and processed. The constant of $20.000.000$ was chosen so that a small, somewhat popular repository gets processed before Linux if Linux was uploaded a little over a week later than the smaller repository.

\newpar{Timeout measure}
On every job there is also a timeout being calculated. This is the amount of time in milliseconds that the database api and controller will allow for this job to take. It is calculated as follows: $$\min \left\{180000 + 5000 \cdot \sqrt{B}, 1800000 + 800000 \cdot \sqrt{S} \right\}$$ Here $B$ is the total amount of parseable bytes and $S$ the total number of stars.

\newpar{Retrieving metadata}
Besides the crawling of GitHub the Crawler is also responsible for retrieving metadata of a project. When given an URL of a project, the Crawler will retrieve the following information about the project:
\begin{itemize}
    \item The current version of the project (the last time a commit was pushed to it);
    \item The license of the project (if available);
    \item The name of the project;
    \item The name of the author of the project;
    \item The e-mail address of the author of the project (if available);
    \item The default branch of the project.
\end{itemize}
It will return this information in a "ProjectMetadata" struct which contains the information specified above. In case of an error (e.g. the url was no longer available) an empty struct is returned.
~\\
~\\
The function responsible for crawling GitHub projects can be called in the \texttt{main} class by calling \texttt{RunCrawler::findMetadata(url, username, token)}
where \texttt{url} is an URL to a GitHub project. We could for example again take \url{https://github.com/torvalds/linux} as input, and we would receive project metadata for this specific project.
~\\
~\\
Do note that in both cases of crawling and retrieving project metadata that we can only do this with GitHub. The structure for extending this does exist in the RunCrawler class, so it should not be too difficult to combine new code supporting other codebases in the future with the existing code.

\newpar{JSON format and the JSON adapter}
In almost every case where we use libcurl to communicate with the GitHub API, we receive data in JSON format. At the moment we use a JSON adapter which uses Nlohmann's JSON library, as we want to be able to easily switch our JSON library. This exists in case you would want a faster JSON library or a more memory efficient JSON library in the future.
\\
\\
The JSON adapter uses a lot of templates: as the Nlohmann's JSON library allows either strings or ints to be used to index on given keys, we have decided that we want to keep this same functionality by specifying what type they are through a template. Since a JSON variable can contain values of several different types (among which are string, integer, and boolean) there is also an "Output" template argument in several functions, which is the type we expect to receive as output. The most important functions in the JSON adapter are the \texttt{get()} and \texttt{branch()} functions, which allow one to retrieve a variable from a JSON structure and branch on a given index respectively. Besides this there are several other functions such as \texttt{isNull()}, \texttt{isEmpty()}, and \texttt{contains()} that allow the user to check if a variable exists in the current branch of the JSON variable, and there are also some helper functions such as \texttt{repeatedGet()} and \texttt{exists()} that prevent code duplication by combining the functions that already exist in the JSON adapter.

\newpar{Error handling}
In most parts of the component where error handling is needed we have some sort of error handling. In the case of our class that communicates with GitHub through the use of libcurl we check the code from GitHub that we received and check if that is an OK code or otherwise an error. In the case of the latter we return either a warning or an error to the user, and if it is a fatal error the Controller is informed. It then makes the decision whether to stop the program or not. Similarly in the JSON adapter we check if there was an error while trying to execute a function (for example, parsing can easily throw an error) and handle accordingly if there indeed was an error. This is most of the times just a fatal error, unless we expected it in which case we throw a warning instead. \\
\\
This error handling works through the use of so called \texttt{ErrorHandler}s, which are classes that contain dictionaries that for each type of error contain an \texttt{IndividualErrorHandler} which has a single function called \texttt{execute()} that handles a given response. We have decided not to make these functions static as we do not want that each class can return an error of every type. For example, the class that handles communication with GitHub should not be able to give JSON errors, as those errors are reserved for the JSON class. However, this approach is sadly not completely without downsides: in the case of the JSON adapter we do not want to create a new JSON error handler for each JSON adapter, and we had to resort to a singleton JSON adapter.
\newpage
\newpar{Viewing data}
As the crawler does not send its data to the console, the data can be viewed by either debugging the program or by adding code that prints the retrieved data itself out. Both the \texttt{crawlRepositories()} and the \texttt{getProjectMetadata()} return the data they've found directly and you can simply print the results you receive from these functions.




\end{document}